import pandas as pd

# Adjust the path if needed
df = pd.read_csv("data/telco_customer_churn.csv")

df.head()
df.shape
df.info()

import numpy as np

# 1. Fix TotalCharges type (often loaded as object)
if "TotalCharges" in df.columns:
    df["TotalCharges"] = pd.to_numeric(df["TotalCharges"], errors="coerce")

# 2. Drop customerID (not useful as a feature)
if "customerID" in df.columns:
    df = df.drop(columns=["customerID"])

# 3. Check missing values
df.isna().sum()
df = df.dropna()
df.shape

target_col = "Churn"

X = df.drop(columns=[target_col])
y = df[target_col].map({"Yes": 1, "No": 0})  # convert to 0/1

X.head(), y.head()
numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()

numeric_features, categorical_features

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

# 1. Train / Test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

X_train.shape, X_test.shape

numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown="ignore")

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ]
)

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, confusion_matrix
)

log_reg_clf = Pipeline(
    steps=[
        ("preprocessor", preprocessor),
        ("model", LogisticRegression(max_iter=1000))
    ]
)

log_reg_clf.fit(X_train, y_train)

y_pred_lr = log_reg_clf.predict(X_test)
y_proba_lr = log_reg_clf.predict_proba(X_test)[:, 1]

print("=== Logistic Regression ===")
print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print("Precision:", precision_score(y_test, y_pred_lr))
print("Recall:", recall_score(y_test, y_pred_lr))
print("F1:", f1_score(y_test, y_pred_lr))
print("ROC-AUC:", roc_auc_score(y_test, y_proba_lr))

# ============================================================
# 1. IMPORTS
# ============================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import joblib

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, confusion_matrix,
    RocCurveDisplay
)


# ============================================================
# 2. LOAD + CLEAN DATA
# ============================================================
df = pd.read_csv("data/telco_customer_churn.csv")

# Fix TotalCharges type issues
df["TotalCharges"] = pd.to_numeric(df["TotalCharges"], errors="coerce")
df = df.dropna()

# Drop ID column if present
if "customerID" in df.columns:
    df = df.drop(columns=["customerID"])


# ============================================================
# 3. TRAIN / TEST SPLIT
# ============================================================
X = df.drop(columns=["Churn"])
y = df["Churn"].map({"Yes": 1, "No": 0})

numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)


# ============================================================
# 4. PREPROCESSING PIPELINE
# ============================================================
preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
    ]
)


# ============================================================
# 5. FINAL RANDOM FOREST MODEL (BEST PARAMS)
# ============================================================
final_rf_clf = Pipeline(
    steps=[
        ("preprocessor", preprocessor),
        ("model", RandomForestClassifier(
            n_estimators=300,
            max_depth=10,
            min_samples_split=10,
            min_samples_leaf=4,
            bootstrap=True,
            class_weight="balanced",
            random_state=42,
            n_jobs=-1
        ))
    ]
)

# Train model
final_rf_clf.fit(X_train, y_train)


# ============================================================
# 6. EVALUATION METRICS
# ============================================================
y_pred = final_rf_clf.predict(X_test)
y_proba = final_rf_clf.predict_proba(X_test)[:, 1]

print("=== FINAL RANDOM FOREST MODEL ===")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1:", f1_score(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, y_proba))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))


# ============================================================
# 7. ROC CURVE
# ============================================================
plt.figure(figsize=(6,5))
RocCurveDisplay.from_predictions(y_test, y_proba)
plt.title("ROC Curve – Final Random Forest")
plt.show()


# ============================================================
# 8. FEATURE IMPORTANCE
# ============================================================
rf_model = final_rf_clf.named_steps["model"]

# Extract OHE categories
ohe = final_rf_clf.named_steps["preprocessor"].transformers_[1][1]
cat_cols = ohe.get_feature_names_out(categorical_features)

# Create feature names list
feature_names = list(numeric_features) + list(cat_cols)
importances = rf_model.feature_importances_

feat_imp = pd.DataFrame({
    "feature": feature_names,
    "importance": importances
}).sort_values("importance", ascending=False)

print("\nTop 20 Most Important Features:")
print(feat_imp.head(20))

# Plot importance
plt.figure(figsize=(12,6))
plt.barh(feat_imp["feature"].head(20), feat_imp["importance"].head(20))
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.title("Top 20 Feature Importances – Random Forest")
plt.gca().invert_yaxis()
plt.show()


# ============================================================
# 9. SAVE MODEL AS PKL
# ============================================================
joblib.dump(final_rf_clf, "final_churn_model.pkl")
print("\nSaved model as final_churn_model.pkl")


# ============================================================
# 10. HOW TO LOAD MODEL LATER
# ============================================================
# model = joblib.load("final_churn_model.pkl")
# model.predict(X_test.head(1))



